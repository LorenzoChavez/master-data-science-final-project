{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       photo                                                url\n",
      "0  000000001  http://s3.amazonaws.com/media.modcloth/images/...\n",
      "1  000000002  http://s3.amazonaws.com/media.modcloth/images/...\n",
      "2  000000003  http://s3.amazonaws.com/media.modcloth/images/...\n",
      "3  000000004  http://s3.amazonaws.com/media.modcloth/images/...\n",
      "4  000000005  http://media1.modcloth.com/community_outfit_im...\n",
      "(424840, 2)\n",
      "(424840,)\n",
      "(424840,)\n"
     ]
    }
   ],
   "source": [
    "# Download urls from www.tamaraberg.com/street2shop/wheretobuyit/photos.tar\n",
    "file_path = \"C:/Users/heret/Desktop/street2shop/photos.txt\"\n",
    "photos_file = pd.read_table(file_path, header=None)\n",
    "\n",
    "photos_file = photos_file[0].str.split(pat=\",\", n=1, expand=True)\n",
    "photos_file.columns = [\"photo\", \"url\"]\n",
    "\n",
    "print(photos_file.head())\n",
    "print(photos_file.shape)\n",
    "print(photos_file[\"photo\"].unique().shape)\n",
    "print(photos_file[\"url\"].unique().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zappos.com             74353\n",
      "amazon.com             63982\n",
      "therealreal.com        55488\n",
      "neimanmarcus.com       28454\n",
      "media.com              23819\n",
      "nordstromimage.com     23434\n",
      "modcloth.com           18435\n",
      "targetimg1.com         17098\n",
      "forever21.com          11390\n",
      "macys.com               9619\n",
      "bloomingdales.com       7258\n",
      "urbanoutfitters.com     6545\n",
      "scene7.com              6255\n",
      "kohls.com               3516\n",
      "anthropologie.com       1942\n",
      "amazonaws.com           1922\n",
      "gap.com                 1874\n",
      "jcrew.com               1664\n",
      "express.com              476\n",
      "Name: 0, dtype: int64\n",
      "357524 images\n"
     ]
    }
   ],
   "source": [
    "# Top url domains\n",
    "url_domains = photos_file[\"url\"].str.extract(r'(\\w*.com)')\n",
    "print(url_domains[0].value_counts().sort_values(ascending=False))\n",
    "print(url_domains[0].value_counts().sort_values(ascending=False).sum(), \"images\") #not all but most included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download images + extract broken urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def image_extraction_3(df):\n",
    "    #we assume a broken link are those taking >3 secs to load up\n",
    "    img_path = \"C:/Users/heret/Desktop/street2shop/photos/\"\n",
    "    urls = df[\"url\"].tolist()\n",
    "    photo_ids = df[\"photo\"].tolist()\n",
    "    broken_urls = pd.DataFrame(columns=[\"photo\", \"url\"]) \n",
    "    print(\"PHOTOS_TEST DIRECTORY\")\n",
    "\n",
    "    for url, photo_id in zip(urls, photo_ids):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=3, \n",
    "                             headers={'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1941.0 Safari/537.36'})\n",
    "            if r.status_code == requests.codes.ok:\n",
    "                with open(str(img_path + photo_id.lstrip(\"0\") + \".jpg\"), 'wb') as f:\n",
    "                    f.write(r.content)\n",
    "        except:\n",
    "            broken_urls = broken_urls.append({\"photo\": photo_id, \"url\": url}, ignore_index=True)\n",
    "    return broken_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_n = 15532 #beware photos ids start at 1 not at 0 as python indexes. ie. start_n = 5216 will download from 5217\n",
    "finish_n = 212420 #ie. finish_n = 5218 will download until 5219 included\n",
    "split_urls = photos_file.loc[start_n:finish_n]\n",
    "#split_urls = photos_file.sort_index(ascending=False).loc[start_n:finish_n] #starting from end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHOTOS_TEST DIRECTORY\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "broken_urls = image_extraction_3(split_urls) #started at 19:21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>photo</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000020005</td>\n",
       "      <td>http://g.nordstromimage.com/imagegallery/store...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000020010</td>\n",
       "      <td>http://g.nordstromimage.com/imagegallery/store...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       photo                                                url\n",
       "0  000020005  http://g.nordstromimage.com/imagegallery/store...\n",
       "1  000020010  http://g.nordstromimage.com/imagegallery/store..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broken_urls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"broken_urls.csv\", \"a\") as f: #always appending to same file, careful not to append same output twice\n",
    "    broken_urls.to_csv(f, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
