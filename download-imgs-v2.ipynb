{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       photo                                                url\n",
      "0  000000001  http://s3.amazonaws.com/media.modcloth/images/...\n",
      "1  000000002  http://s3.amazonaws.com/media.modcloth/images/...\n",
      "2  000000003  http://s3.amazonaws.com/media.modcloth/images/...\n",
      "3  000000004  http://s3.amazonaws.com/media.modcloth/images/...\n",
      "4  000000005  http://media1.modcloth.com/community_outfit_im...\n",
      "(424840, 2)\n",
      "(424840,)\n",
      "(424840,)\n"
     ]
    }
   ],
   "source": [
    "# Download urls from www.tamaraberg.com/street2shop/wheretobuyit/photos.tar\n",
    "file_path = \"C:/Users/heret/Desktop/street2shop/photos.txt\"\n",
    "photos_file = pd.read_table(file_path, header=None)\n",
    "\n",
    "photos_file = photos_file[0].str.split(pat=\",\", n=1, expand=True)\n",
    "photos_file.columns = [\"photo\", \"url\"]\n",
    "\n",
    "print(photos_file.head())\n",
    "print(photos_file.shape)\n",
    "print(photos_file[\"photo\"].unique().shape)\n",
    "print(photos_file[\"url\"].unique().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zappos.com             74353\n",
      "amazon.com             63982\n",
      "therealreal.com        55488\n",
      "neimanmarcus.com       28454\n",
      "media.com              23819\n",
      "nordstromimage.com     23434\n",
      "modcloth.com           18435\n",
      "targetimg1.com         17098\n",
      "forever21.com          11390\n",
      "macys.com               9619\n",
      "bloomingdales.com       7258\n",
      "urbanoutfitters.com     6545\n",
      "scene7.com              6255\n",
      "kohls.com               3516\n",
      "anthropologie.com       1942\n",
      "amazonaws.com           1922\n",
      "gap.com                 1874\n",
      "jcrew.com               1664\n",
      "express.com              476\n",
      "Name: 0, dtype: int64\n",
      "357524 images\n"
     ]
    }
   ],
   "source": [
    "# Top url domains\n",
    "url_domains = photos_file[\"url\"].str.extract(r'(\\w*.com)')\n",
    "print(url_domains[0].value_counts().sort_values(ascending=False))\n",
    "print(url_domains[0].value_counts().sort_values(ascending=False).sum(), \"images\") #not all but most included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download images + extract broken urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def image_extraction_3(df):\n",
    "    #we assume a broken link are those taking >3 secs to load up\n",
    "    img_path = \"C:/Users/heret/Desktop/street2shop/photos/\"\n",
    "    urls = df[\"url\"].tolist()\n",
    "    photo_ids = df[\"photo\"].tolist()\n",
    "    \n",
    "    broken_urls = pd.DataFrame(columns=[\"photo\", \"url\"])\n",
    "    with open(\"broken_urls_2.csv\", \"a\") as f:\n",
    "        broken_urls.to_csv(f, index=False)\n",
    "\n",
    "    for url, photo_id in zip(urls, photo_ids):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=3, \n",
    "                             headers={'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1941.0 Safari/537.36'})\n",
    "            if r.status_code == requests.codes.ok:\n",
    "                with open(str(img_path + photo_id.lstrip(\"0\") + \".jpg\"), 'wb') as f:\n",
    "                    f.write(r.content)\n",
    "        except:\n",
    "            with open(\"broken_urls_2.csv\", \"a\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([photo_id, url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_n = 18893 #beware photos ids start at 1 not at 0 as python indexes. ie. start_n = 5216 will download from 5217\n",
    "finish_n = 212420 #ie. finish_n = 5218 will download until 5219 included\n",
    "split_urls = photos_file.loc[start_n:finish_n]\n",
    "#split_urls = photos_file.sort_index(ascending=False).loc[start_n:finish_n] #starting from end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "image_extraction_3(split_urls) #started at 22:05 #DO NOT OPEN CSV FILE WHILE SCRIPT IS RUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
