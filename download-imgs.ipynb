{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using this notebook to download the dataset images\n",
    "The final dataset comprises over 400k pictures taking about 200GB on disk.\n",
    "\n",
    "Downloading the whole dataset takes several days since the images are available through a txt file with direct links to the websites hosting the images. \n",
    "\n",
    "Having the whole dataset not available through direct download results in a series of issues since some of these links are not working anymore and some other typical issues arise such as corrupted images, images having been replaced for an error placeholder image or antiscraping protection. These issues will be explored on separate notebooks, finally for all those images that are not available anymore, a csv file has been pulled together and the team behind the street2go dataset has been contacted for help in regards to these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       photo                                                url\n",
      "0  000000001  http://s3.amazonaws.com/media.modcloth/images/...\n",
      "1  000000002  http://s3.amazonaws.com/media.modcloth/images/...\n",
      "2  000000003  http://s3.amazonaws.com/media.modcloth/images/...\n",
      "3  000000004  http://s3.amazonaws.com/media.modcloth/images/...\n",
      "4  000000005  http://media1.modcloth.com/community_outfit_im...\n",
      "(424840, 2)\n",
      "(424840,)\n",
      "(424840,)\n"
     ]
    }
   ],
   "source": [
    "# Download urls from www.tamaraberg.com/street2shop/wheretobuyit/photos.tar\n",
    "file_path = \"./photos.txt\"\n",
    "photos_file = pd.read_table(file_path, header=None)\n",
    "\n",
    "photos_file = photos_file[0].str.split(pat=\",\", n=1, expand=True)\n",
    "photos_file.columns = [\"photo\", \"url\"]\n",
    "\n",
    "print(photos_file.head())\n",
    "print(photos_file.shape)\n",
    "print(photos_file[\"photo\"].unique().shape)\n",
    "print(photos_file[\"url\"].unique().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.zappos.com                     74353\n",
      "ecx.images-amazon.com              45773\n",
      "images.asos-media.com              23819\n",
      "g.nordstromimage.com               23434\n",
      "media1.modcloth.com                18435\n",
      "g-ecx.images-amazon.com            18209\n",
      "product-images2.therealreal.com    17125\n",
      "scene7.targetimg1.com              17098\n",
      "productshots2.modcloth.net         16942\n",
      "product-images4.therealreal.com    16911\n",
      "productshots3.modcloth.net         16875\n",
      "productshots1.modcloth.net         16768\n",
      "productshots0.modcloth.net         16731\n",
      "images.neimanmarcus.com            15197\n",
      "www.neimanmarcus.com               13257\n",
      "www.forever21.com                  11390\n",
      "product-images1.therealreal.com    10761\n",
      "product-images3.therealreal.com    10691\n",
      "slimages.macys.com                  9619\n",
      "images.bloomingdales.com            7258\n",
      "images.urbanoutfitters.com          6545\n",
      "s7d2.scene7.com                     6255\n",
      "media.kohls.com.edgesuite.net       3516\n",
      "images.anthropologie.com            1942\n",
      "s3.amazonaws.com                    1922\n",
      "s7.jcrew.com                        1664\n",
      "bananarepublic.gap.com               884\n",
      "oldnavy.gap.com                      539\n",
      "images.express.com                   476\n",
      "www.gap.com                          236\n",
      "piperlime.gap.com                    215\n",
      "Name: 0, dtype: int64\n",
      "424840 images\n"
     ]
    }
   ],
   "source": [
    "# Top url domains\n",
    "url_domains = photos_file[\"url\"].str.extract(r'(?:^https?:\\/\\/([^\\/]+)(?:[\\/,]|$)|^(.*)$)')\n",
    "print(url_domains[0].value_counts().sort_values(ascending=False))\n",
    "print(url_domains[0].value_counts().sort_values(ascending=False).sum(), \"images\") #not all but most included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download images + extract broken urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def image_extraction_3(df):\n",
    "    #we assume a broken link are those taking >3 secs to load up\n",
    "    img_path = \"C:/Users/heret/Desktop/street2shop/photos/\"\n",
    "    urls = df[\"url\"].tolist()\n",
    "    photo_ids = df[\"photo\"].tolist()\n",
    "    \n",
    "    broken_urls = pd.DataFrame(columns=[\"photo\", \"url\"])\n",
    "    with open(\"broken_urls.csv\", \"a\") as f:\n",
    "        broken_urls.to_csv(f, index=False)\n",
    "\n",
    "    for url, photo_id in zip(urls, photo_ids):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=3, \n",
    "                             headers={'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1941.0 Safari/537.36'})\n",
    "            if r.status_code == requests.codes.ok:\n",
    "                with open(str(img_path + photo_id.lstrip(\"0\") + \".jpg\"), 'wb') as f:\n",
    "                    f.write(r.content)\n",
    "        except:\n",
    "            with open(\"broken_urls.csv\", \"a\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([photo_id, url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_n = 258511 #beware photos ids start at 1 not at 0 as python indexes. ie. start_n = 5216 will download from 5217\n",
    "finish_n = 275000 #ie. finish_n = 5218 will download until 5219 included\n",
    "split_urls = photos_file.loc[start_n:finish_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5h 15min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "image_extraction_3(split_urls) #started at 22:05 #DO NOT OPEN CSV FILE WHILE SCRIPT IS RUNNING"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
