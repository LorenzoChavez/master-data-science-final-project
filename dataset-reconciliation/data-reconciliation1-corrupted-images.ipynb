{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrupted images\n",
    "On this notebook I will explore the corrupted images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import requests\n",
    "import urllib\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying corrupted images\n",
    "I will attempt to open all images and capture the exceptions to identify the corrupted images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupted_imgs(img_paths):\n",
    "    corrupted = pd.DataFrame(columns=[\"filename\"])\n",
    "    for img_path in img_paths:\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            img.verify()\n",
    "        except (IOError, SyntaxError) as e:\n",
    "            corrupted = corrupted.append({\"filename\":img_path.split(\"/\")[-1]}, ignore_index=True)\n",
    "    corrupted.to_csv(\"corrupted_photos.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 474 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path_photos = \"./photos/\"\n",
    "img_paths = [path_photos + img for img in os.listdir(path_photos)]\n",
    "corrupted_imgs(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads previous file and joins it to the file having all the urls\n",
    "def corrupted_urls(urls, corrupted_ids):\n",
    "    photos_file = pd.read_table(urls, header=None)\n",
    "    photos_file = photos_file[0].str.split(pat=\",\", n=1, expand=True)\n",
    "    photos_file.columns = [\"photo\", \"url\"]\n",
    "    \n",
    "    corrupted = pd.read_csv(corrupted_ids)\n",
    "    print(corrupted.shape[0], \"corrupted images\")\n",
    "    \n",
    "    corrupted[\"filename\"] = corrupted[\"filename\"].str.replace(\".jpg\", \"\")\n",
    "    photos_file[\"photo\"] = photos_file[\"photo\"].str.lstrip(\"0\")\n",
    "    corrupted_df = pd.merge(corrupted, photos_file, how=\"inner\", left_on=[\"filename\"], right_on=[\"photo\"])\n",
    "    url_domains = corrupted_df[\"url\"].str.extract(r'(?:^https?:\\/\\/([^\\/]+)(?:[\\/,]|$)|^(.*)$)')\n",
    "    print(url_domains[0].value_counts().sort_values(ascending=False))\n",
    "    \n",
    "    return corrupted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364 corrupted images\n",
      "piperlime.gap.com    215\n",
      "www.forever21.com    148\n",
      "s3.amazonaws.com       1\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "url_path = \"./photos.txt\" #path where the photos.txt file is located\n",
    "corrupted = \"./corrupted_photos.csv\" #file location from where the previous function's output was saved to\n",
    "corrupted_df = corrupted_urls(url_path, corrupted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>photo</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100676</td>\n",
       "      <td>100676</td>\n",
       "      <td>http://www.forever21.com/images/7_additional_7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100679</td>\n",
       "      <td>100679</td>\n",
       "      <td>http://www.forever21.com/images/7_additional_7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100680</td>\n",
       "      <td>100680</td>\n",
       "      <td>http://www.forever21.com/images/2_side_750/001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100986</td>\n",
       "      <td>100986</td>\n",
       "      <td>http://www.forever21.com/images/2_side_750/001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100987</td>\n",
       "      <td>100987</td>\n",
       "      <td>http://www.forever21.com/images/1_front_750/00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  filename   photo                                                url\n",
       "0   100676  100676  http://www.forever21.com/images/7_additional_7...\n",
       "1   100679  100679  http://www.forever21.com/images/7_additional_7...\n",
       "2   100680  100680  http://www.forever21.com/images/2_side_750/001...\n",
       "3   100986  100986  http://www.forever21.com/images/2_side_750/001...\n",
       "4   100987  100987  http://www.forever21.com/images/1_front_750/00..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrupted_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Downloading with urllib corrupted images\n",
    "For some reason the image_extraction function used on the download notebook corrupts some of the images, the same function has now been tweeked to allow downloading with the urllib library which does not corrupt any images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attempting download with either requests or urllib\n",
    "def image_extraction(df, requests_on=True):\n",
    "    \"\"\"\n",
    "    df = needs to be a dataframe with the url and photo columns\n",
    "    \"\"\"\n",
    "\n",
    "    img_path = \"./photos_corrupted/\"\n",
    "    urls = df[\"url\"].tolist()\n",
    "    photo_ids = df[\"photo\"].tolist()\n",
    "    \n",
    "    broken_urls = pd.DataFrame(columns=[\"photo\", \"url\"])\n",
    "    with open(\"broken_urls_corrupted.csv\", \"a\") as f:\n",
    "        broken_urls.to_csv(f, index=False)\n",
    "\n",
    "    for url, photo_id in zip(urls, photo_ids):\n",
    "        try:\n",
    "            if requests_on == True:\n",
    "                r = requests.get(url, timeout=3, \n",
    "                                 headers={\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1941.0 Safari/537.36\"})\n",
    "                if r.status_code == requests.codes.ok:\n",
    "                    with open(str(img_path + photo_id.lstrip(\"0\") + \".jpg\"), \"wb\") as f:\n",
    "                        f.write(r.content)\n",
    "            else:\n",
    "                urllib.request.urlretrieve(url, img_path + photo_id.lstrip(\"0\") + \".jpg\")\n",
    "        except:\n",
    "            with open(\"broken_urls_corrupted.csv\", \"a\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([photo_id, url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_n = 0 #beware photos ids start at 1 not at 0 as python indexes. ie. start_n = 5216 will download from 5217\n",
    "#finish_n = 8000 #ie. finish_n = 5218 will download until 5219 included\n",
    "split_urls = corrupted_df.loc[start_n:finish_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "image_extraction(split_urls, requests=False) #DO NOT OPEN CSV FILE WHILE SCRIPT IS RUNNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When downloading with urllib now some urls are showing as broken but they are not\n",
    "\n",
    "Some websites seem to have protection against scraping, in these cases the functions with requests works better, switching between urllib and requests until all corrupted images are correctly downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_urls = pd.read_csv(\"./broken_urls_corrupted.csv\")\n",
    "broken_urls = broken_urls.dropna(axis=0)\n",
    "broken_urls[\"photo\"] = broken_urls[\"photo\"].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_n = 1062 #beware photos ids start at 1 not at 0 as python indexes. ie. start_n = 5216 will download from 5217\n",
    "# finish_n =  #ie. finish_n = 5218 will download until 5219 included\n",
    "split_urls = broken_urls.loc[start_n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "image_extraction(split_urls, requests_on=True) #DO NOT OPEN CSV FILE WHILE SCRIPT IS RUNNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running again from the beggining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "path_photos = \"./photos/\"\n",
    "img_paths = [path_photos + img for img in os.listdir(path_photos)]\n",
    "corrupted_imgs(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364 corrupted images\n",
      "piperlime.gap.com    215\n",
      "www.forever21.com    148\n",
      "s3.amazonaws.com       1\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "url_path = \"./photos.txt\"\n",
    "corrupted = \"./corrupted_photos.csv\"\n",
    "corrupted_df = corrupted_urls(url_path, corrupted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images from piperlime and amazon are broken urls while images from forever21 are still corrupted.  \n",
    "Will download forever21 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_df = corrupted_df[corrupted_df[\"url\"].str.contains(\"forever21\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['100676', '100676',\n",
       "        'http://www.forever21.com/images/7_additional_750/00101719-04.jpg'],\n",
       "       ['100679', '100679',\n",
       "        'http://www.forever21.com/images/7_additional_750/00118396-03.jpg']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrupted_df[:2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_n = 0 #beware photos ids start at 1 not at 0 as python indexes. ie. start_n = 5216 will download from 5217\n",
    "#finish_n = 2 #ie. finish_n = 5218 will download until 5219 included\n",
    "split_urls = corrupted_df.loc[start_n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "image_extraction(split_urls, requests_on=False) #DO NOT OPEN CSV FILE WHILE SCRIPT IS RUNNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *piperlime.gap* & *amazon* urls despite downloading a jpg file are actually genuine broken urls, they will be double checked on the last reconciliation notebook, so for now we will leave them on the dataset folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
