{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broken urls\n",
    "On this notebook I will explore the broken urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import urllib\n",
    "import csv\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same function from previous notebooks to download the images.  \n",
    "\n",
    "Some of these images are available and will download with either urllib or requests. There seems to be a certain randomness when downloading these images so multiple attempts are performed, this is possibly due to antiscraping protection from som of this websites.  \n",
    "\n",
    "The main goal is to download as many images as possible and request any left images to the team behind the original street2shop dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attempting download with either requests or urllib\n",
    "def image_extraction(df, requests_on=True):\n",
    "    \"\"\"\n",
    "    df = needs to be a dataframe with the url and photo columns\n",
    "    \"\"\"\n",
    "    img_path = \"./photos_v2/\"\n",
    "    urls = df[\"url\"].tolist()\n",
    "    photo_ids = df[\"photo\"].tolist()\n",
    "    \n",
    "    broken_urls = pd.DataFrame(columns=[\"photo\", \"url\"])\n",
    "    with open(\"broken_urls.csv\", \"a\") as f:\n",
    "        broken_urls.to_csv(f, index=False)\n",
    "\n",
    "    for url, photo_id in zip(urls, photo_ids):\n",
    "        try:\n",
    "            if requests_on == True:\n",
    "                r = requests.get(url, timeout=5, \n",
    "                                 headers={\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1941.0 Safari/537.36\"})\n",
    "                if r.status_code == requests.codes.ok:\n",
    "                    with open(str(img_path + photo_id + \".jpg\"), \"wb\") as f:\n",
    "                        f.write(r.content)\n",
    "            else:\n",
    "                urllib.request.urlretrieve(url, img_path + str(photo_id) + \".jpg\")\n",
    "        except:\n",
    "            with open(\"broken_urls.csv\", \"a\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([photo_id, url])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing images\n",
    "Rather thank using a csv with broken links I will use the function below to check which images are missing in the dataset folder, this allows for easier and more robust reconciliation.  \n",
    "\n",
    "This process is repeated multiple times, for efficiency targeting each url domain at a time and making attempts with both requests and urllib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_images(list_pics, n):    \n",
    "    all_ids = [str(n)+\".jpg\" for n in range(n)]\n",
    "    missing = list(set(all_ids) - set(list_pics)) #this will compare all images up to n with the missing ones in the folder\n",
    "    print(\"Total photos in folder {}, Total photos missing {}\".format(len(list_pics), len(missing)))\n",
    "    \n",
    "    missing_df = pd.DataFrame(missing, columns=[\"filename\"])\n",
    "    missing_df[\"filename\"] = missing_df[\"filename\"].str.replace(\".jpg\", \"\")\n",
    "    photos_file = pd.read_table(\"./photos.txt\", header=None) #now loading the file with the urls so can join them up later\n",
    "    photos_file = photos_file[0].str.split(pat=\",\", n=1, expand=True)\n",
    "    photos_file.columns = [\"photo\", \"url\"]\n",
    "    photos_file[\"photo\"] = photos_file[\"photo\"].str.lstrip(\"0\")\n",
    "\n",
    "    all_missing_df = pd.merge(missing_df, photos_file, how=\"inner\", left_on=[\"filename\"], right_on=[\"photo\"])\n",
    "    return all_missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total photos in folder 259430, Total photos missing 33955\n"
     ]
    }
   ],
   "source": [
    "list_pics = os.listdir(\"./photos\")\n",
    "n_pics = 291050\n",
    "missing = missing_images(list_pics, n_pics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g.nordstromimage.com             20376\n",
      "images.bloomingdales.com          6343\n",
      "productshots1.modcloth.net        1788\n",
      "productshots0.modcloth.net        1746\n",
      "productshots2.modcloth.net        1745\n",
      "productshots3.modcloth.net        1724\n",
      "www.forever21.com                  188\n",
      "media.kohls.com.edgesuite.net       42\n",
      "images.express.com                   1\n",
      "ecx.images-amazon.com                1\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "url_domains = missing[\"url\"].str.extract(r'(?:^https?:\\/\\/([^\\/]+)(?:[\\/,]|$)|^(.*)$)')\n",
    "print(url_domains[0].value_counts().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_2 = broken_urls_all[~broken_urls_all[\"url\"].str.contains(\"nordstrom\")] #nordstrom all are broken\n",
    "#urls_modcloth = urls_2[urls_2[\"url\"].str.contains(\"modcloth\")] #downloading modcloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_n = 0 #beware photos ids start at 1 not at 0 as python indexes. ie. start_n = 5216 will download from 5217\n",
    "#finish_n = 2 #ie. finish_n = 5218 will download until 5219 included\n",
    "split_urls = missing.loc[start_n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "image_extraction(split_urls, requests_on=False) #DO NOT OPEN CSV FILE WHILE SCRIPT IS RUNNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once satified with the amount of images downloaded the remaining ones are saved to a csv file to be send to the street2shop team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing.sort_values(by=\"photo\").drop(columns=[\"filename\"]).to_csv(\"broken_urls.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
